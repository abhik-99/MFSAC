{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Univariate Supervised Gene Clustering.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoQ2hwodTPmM+sm2vChAJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhik-99/MFSGC/blob/master/Univariate_Supervised_Gene_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lW6QtcqxDye",
        "colab_type": "text"
      },
      "source": [
        "#Univariate Supervised Gene Clustering\n",
        "**UFSGC** is a method where by a specific filter algorithm is used to score and filter out high ranking genes from Gene Expression Dataset and then the filtered Genes are put through SGC for Gene Augmentation. The resulting Augmentation not only increases the class separability of the genes but also their expressions.\\\n",
        "This Augmented gene expression set is now used for classification of cancer from healthy patients.\\\n",
        "The Filter Methods chosen for evaluation are:- \n",
        "1. Mutual Information.\n",
        "2. ReliefF.\n",
        "3. Chi Sq.\n",
        "4. Fisher Score.\n",
        "5. Signal To Noise Ratio (adapted for multi-class datasets).\n",
        "6. T-Test.\n",
        "7. Pearson Corelation Coefficient.\n",
        "\n",
        "This method is used for evaluation of **MFSGC**.\\\n",
        "\\\n",
        "*If you already have Gene Representatives from a previous iteration, you can load them and use them here. Loading can be done using the last two cells of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgq-QT-l3LS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install skfeature-chappers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3I4n8fdw70p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import chi2\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuXPXJTuIPrw",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWEU4ygS3PiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2. Get the file\n",
        "downloaded = drive.CreateFile({'id':'1oaOATE0D_f8MGPIMJOMXYVt0hBUWNCKV'}) # replace the id with id of file you want to access\n",
        "# For Leukemia- 1xcL-LT-E_gUqWLlqqeVJP1DVHHpiAGe_\n",
        "# For Colon - 1AUOto0GhTHW9fX52XSsf9kzYJS5ggv0G\n",
        "# for Prostate - 13Hf7uGbyJ1sWYo8KDRDL8scm-2Fs9_gd\n",
        "# For Lung- 1xuLzTWDGUbr4x3Pq1dnJj08MZqBB5I3U\n",
        "# for Rahc - 1oaOATE0D_f8MGPIMJOMXYVt0hBUWNCKV\n",
        "# for Raoa - 1d2vhPcT3I7ZFcAGOQYVLGB3Jx_vEMata\n",
        "# for Rbreast - 1Vf-h8zfVP_twMXivcJJtbWtjThShUHvn\n",
        "# for SRBCT - 1rO5EEvsoRJl2VVUB3ywKUd3kNiQ24oy3\n",
        "# for MLL - 1rS7x4x_DhrUzaBhrgKMQH3uIaLJdPgW3\n",
        "# for Breast - 1enhhyA4u2ByvOjnF81WoHflVNpXtfKpu\n",
        "downloaded.GetContentFile('data.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NtuVbjd3QZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DATASET is the name of the dataset being used.\n",
        "DATASET=\"RAHC\"\n",
        "\n",
        "#NEIGHBOURS determines neighbours arg for ReliefF\n",
        "#for any dataset which contains any class sample \n",
        "# <10, make it less than 10. Eg of such dataset - SRBCT\n",
        "NEIGHBOURS = 3 \n",
        "\n",
        "#p is the number of top genes taken after sorting the filter scores\n",
        "p = 800\n",
        "\n",
        "#q is the number of top genes to be taken from each filter after augmentation\n",
        "q = 5\n",
        "\n",
        "#uncomment the line below if using the dataset splitter else leave it commented \n",
        "#data_df = pd.read_csv(\"%s_train.csv\"%(DATASET),index_col=0)\n",
        "\n",
        "#uncomment the lines below if using the original dataset\n",
        "dataset = pd.read_table(\"data.txt\",header=None)\n",
        "data_df = dataset\n",
        "\n",
        "\n",
        "\n",
        "target = data_df.iloc[:,-1]\n",
        "feature = pd.DataFrame(data_df.iloc[:,:-1].values,dtype='float')\n",
        "m,n = feature.shape\n",
        "print(m,n)\n",
        "print(feature.head())\n",
        "print(\"Number of classes - \")\n",
        "classes = np.unique(target)\n",
        "for x in classes:\n",
        "  print(\"Class -\",x,\"Number of Sampples -\", len(np.where(target == x)[0]))\n",
        "\n",
        "feature_norm=pd.DataFrame(MinMaxScaler().fit_transform(feature))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvHtoWSI3jFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#utility function\n",
        "def plot_feature(feature, target, c = ['r', 'b', 'g', 'y']):\n",
        "  import matplotlib.pyplot as plt\n",
        "  from matplotlib import style\n",
        "  import numpy as np\n",
        "  style.use('ggplot')\n",
        "  for idx, each in enumerate(np.unique(target)):\n",
        "    y = feature[np.where(target == each)[0]]\n",
        "    x = len(y)\n",
        "    plt.scatter(range(1, x+1), y, color = c[idx])\n",
        "    plt.plot(range(1, x+1), y, color = c[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXYQfsObIRy_",
        "colab_type": "text"
      },
      "source": [
        "## Creating Filter Methods for Scoring and filtering top rated genes\n",
        "The Filter Methods chosen for evaluation are:-\n",
        "\n",
        "Mutual Information.\n",
        "ReliefF.\n",
        "Chi Sq.\n",
        "Fisher Score.\n",
        "Signal To Noise Ratio (adapted for multi-class datasets).\n",
        "T-Test.\n",
        "Pearson Corelation Coefficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqPoVNzW3oVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#construction of ReliefF function\n",
        "\n",
        "\"\"\"\n",
        "Given a dataset, number of random instances to pick form the dataset and\n",
        "number of features to consider in each iteration (k), the function returns the weigths of the attributes\n",
        "of the dataset.\n",
        "These weigths can then be used as the final results out of the ReliefF algorithm\n",
        "\n",
        "Paper-\n",
        "\n",
        "Marko Robnik-ˇSikonja and Igor Kononenko. Theoretical and empirical analysis of relieff\n",
        "and rrelieff. Machine learning, 53(1-2):23–69, 2003.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def hit_miss_calculator(target,instance,k = 10, hit = True, c = None, ):\n",
        "    m=len(target)\n",
        "    upper,lower=instance-1,instance+1\n",
        "    hits=[]\n",
        "    hit_flag=False\n",
        "    #finds k nearest hits\n",
        "    while(not hit_flag):\n",
        "      #print(upper,lower)\n",
        "      if(len(hits)>=k):\n",
        "        hit_flag = True\n",
        "        break\n",
        "      if upper < 0 and lower > m:\n",
        "        hit_flag = True\n",
        "        break\n",
        "      if(upper>=0):\n",
        "        if((target[upper]==target[instance]) and hit):\n",
        "          hits.append(upper)\n",
        "        elif((target[upper]!=target[instance]) and (not hit) and target[upper]==c):\n",
        "          hits.append(upper)\n",
        "        upper-=1          \n",
        "      if(lower<m):\n",
        "        if((target[lower]==target[instance]) and hit):\n",
        "          hits.append(lower)\n",
        "        elif((target[lower]!=target[instance]) and (not hit) and target[lower]==c):\n",
        "          hits.append(lower)\n",
        "        lower+=1\n",
        "    hits.sort()\n",
        "    return hits\n",
        "\n",
        "\n",
        "def reliefF(feature,target,k=10,repetitions=10, seed = 0):\n",
        "  np.random.seed(seed)\n",
        "  if len(feature.shape)>1:\n",
        "    m,n=feature.shape\n",
        "  else:\n",
        "    m=len(feature)\n",
        "    n=1\n",
        "  #print(m,n)\n",
        "  observations=list(range(m))\n",
        "  classes=np.unique(target)\n",
        "  weights=np.zeros(n)\n",
        "  d=(np.max(feature,axis=0)-np.min(feature,axis=0))*m*k\n",
        "\n",
        "  for i in range(repetitions):\n",
        "    instance=np.random.choice(observations,1)[0]\n",
        "    #print(\"Iteration\",i)\n",
        "    #print(instance)\n",
        "    hits=hit_miss_calculator(target,instance,k)\n",
        "    hit_class_prob=len(np.where(target==target[instance])[0])/m\n",
        "    #print(\"\\nHit Probability -\",hit_class_prob)\n",
        "    #print(\"Repetition\",i,\"Class\",target[instance],\"Hits -\",hits)\n",
        "\n",
        "    miss={}\n",
        "    miss_class_prob={}\n",
        "\n",
        "    for each_class in classes:\n",
        "      if(each_class != target[instance]):\n",
        "        miss[each_class]=hit_miss_calculator(target,instance,k,False,each_class)\n",
        "        class_prob=len(np.where(target==each_class)[0])/m\n",
        "        #print(each_class,class_prob)\n",
        "        miss_class_prob[each_class]=hit_class_prob/(1 - (class_prob))\n",
        "\n",
        "    #print(\"Repetition\",i,\"Miss-\",miss,\"Miss Class Probability -\",miss_class_prob)\n",
        "    \n",
        "    for hit in hits:\n",
        "      if len(feature.shape)>1:\n",
        "        weights-=np.subtract(feature.iloc[instance,:],feature.iloc[hit,:])/d\n",
        "      else:\n",
        "        weights-=np.subtract(feature.iloc[instance],feature.iloc[hit])/d\n",
        "    for each_class in miss:\n",
        "      for each_miss in miss[each_class]:\n",
        "        if len(feature.shape)>1:\n",
        "          weights+=(np.subtract(feature.iloc[instance,:],feature.iloc[each_miss,:])/d)*miss_class_prob[each_class]\n",
        "        else:\n",
        "          weights+=(np.subtract(feature.iloc[instance],feature.iloc[each_miss])/d)*miss_class_prob[each_class]\n",
        "    \n",
        "    \n",
        "  return weights.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM_OrCSa3ro0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function discretizes the given features into 3 categories\n",
        "def discretize_feature(feature):\n",
        "  \n",
        "  mean=np.mean(feature)\n",
        "  std=np.std(feature)\n",
        "  discretized=np.copy(feature)\n",
        "  \n",
        "  discretized[np.where(feature<(mean+std/2)) ,]=2#within 1/2 std div\n",
        "  discretized[np.where(feature>(mean-std/2)),]=2#within 1/2 std div\n",
        "  \n",
        "  discretized[np.where(feature>(mean+std/2)),]=0#greater than half\n",
        "  discretized[np.where(feature<(mean-std/2)),]=1#less than half\n",
        "  \n",
        "  return discretized\n",
        "\n",
        "def Xfreq(x):\n",
        "  xL={}\n",
        "  for e in x:\n",
        "    if e not in xL:\n",
        "      xL[e]=0\n",
        "    else:\n",
        "      xL[e]+=1\n",
        "  for e in xL:\n",
        "    xL[e]/=len(x)\n",
        "  return xL\n",
        "\n",
        "def XYfreq(x,y):\n",
        "  freq={}\n",
        "  \n",
        "  rX=np.unique(x)\n",
        "  rY=np.unique(y)\n",
        "      \n",
        "  for e in rX:\n",
        "    for f in rY:\n",
        "      freq[(e,f)]=round(len(np.where(y[np.where(x==e)[0]]==f)[0])/len(x),4)\n",
        "       \n",
        "  return freq\n",
        "\n",
        "def mutual_info(x,y):\n",
        "\n",
        "  xFreq=Xfreq(x)\n",
        "  yFreq=Xfreq(y)\n",
        "  joint=XYfreq(x,y)\n",
        "  \n",
        "  Xentropy=0\n",
        "  for e in xFreq:\n",
        "    if xFreq[e]!=0:\n",
        "      Xentropy-=xFreq[e]*np.log2(xFreq[e])\n",
        "      \n",
        "  Yentropy=0\n",
        "  for e in yFreq:\n",
        "    if yFreq[e]!=0:\n",
        "      Yentropy-=yFreq[e]*np.log2(yFreq[e])\n",
        "      \n",
        "  jentropy=0\n",
        "  for e in xFreq:\n",
        "    for f in yFreq:\n",
        "      if joint[(e,f)]!=0:\n",
        "        jentropy-=joint[(e,f)]*np.log2(joint[(e,f)])\n",
        "  \n",
        "  return (Xentropy+Yentropy-jentropy)\n",
        "\n",
        "def mutual_info_wrapper(features,target):\n",
        "\n",
        "  mi=np.array([])\n",
        "  for x in features:\n",
        "    discrete=discretize_feature(features[x])\n",
        "    mi=np.append(mi,mutual_info(discrete,target))\n",
        "  return np.array(mi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1E5U3Fe3uQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This cell is used for defining the method for calculating the t-scores\n",
        "\"\"\"\n",
        "\n",
        "def t_test(df,target):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  df= Dataframe of features (n_samples,n_features)\n",
        "  target= Pandas Series/1D Numpy Array containing the class labels (n_samples)\n",
        "  \n",
        "  Output:\n",
        "  scores= Descendingly Sorted array of features based on t-test \n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  from scipy.stats import ttest_ind\n",
        "  scores=ttest_ind(df[:][target==0],df[:][target==1])[0] #Storing just the t-test scores and discarding the p-values from the result.\n",
        "  \n",
        "  # scores=np.argsort(scores,0)\n",
        "  return [scores] if type(scores) != np.ndarray else scores\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RktVmEeu3w_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import *\n",
        "def fisher_score(X, y):\n",
        "    import numpy as np\n",
        "    \n",
        "    from skfeature.utility.construct_W import construct_W\n",
        "    \"\"\"\n",
        "    This function implements the fisher score feature selection, steps are as follows:\n",
        "    1. Construct the affinity matrix W in fisher score way\n",
        "    2. For the r-th feature, we define fr = X(:,r), D = diag(W*ones), ones = [1,...,1]', L = D - W\n",
        "    3. Let fr_hat = fr - (fr'*D*ones)*ones/(ones'*D*ones)\n",
        "    4. Fisher score for the r-th feature is score = (fr_hat'*D*fr_hat)/(fr_hat'*L*fr_hat)-1\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    X: {numpy array}, shape (n_samples, n_features)\n",
        "        input data\n",
        "    y: {numpy array}, shape (n_samples,)\n",
        "        input class labels\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    score: {numpy array}, shape (n_features,)\n",
        "        fisher score for each feature\n",
        "\n",
        "    Reference\n",
        "    ---------\n",
        "    He, Xiaofei et al. \"Laplacian Score for Feature Selection.\" NIPS 2005.\n",
        "    Duda, Richard et al. \"Pattern classification.\" John Wiley & Sons, 2012.\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct weight matrix W in a fisherScore way\n",
        "    kwargs = {\"neighbor_mode\": \"supervised\", \"fisher_score\": True, 'y': y}\n",
        "    W = construct_W(X, **kwargs)\n",
        "\n",
        "    # build the diagonal D matrix from affinity matrix W\n",
        "    D = np.array(W.sum(axis=1))\n",
        "    L = W\n",
        "    tmp = np.dot(np.transpose(D), X)\n",
        "    D = diags(np.transpose(D), [0])\n",
        "    Xt = np.transpose(X)\n",
        "    t1 = np.transpose(np.dot(Xt, D.todense()))\n",
        "    t2 = np.transpose(np.dot(Xt, L.todense()))\n",
        "    # compute the numerator of Lr\n",
        "    D_prime = np.sum(np.multiply(t1, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
        "    # compute the denominator of Lr\n",
        "    L_prime = np.sum(np.multiply(t2, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
        "    # avoid the denominator of Lr to be 0\n",
        "    D_prime[D_prime < 1e-12] = 10000\n",
        "    lap_score = 1 - np.array(np.multiply(L_prime, 1/D_prime))[0, :]\n",
        "\n",
        "    # compute fisher score from laplacian score, where fisher_score = 1/lap_score - 1\n",
        "    score = 1.0/lap_score - 1\n",
        "    return np.transpose(score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acdy9Tp_3zzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Pearson corelation\n",
        "def pearson_corr(feature,targetClass):\n",
        "  import numpy as np\n",
        "  coef=[np.abs(np.corrcoef(feature[i].values,targetClass)[0,1]) for i in feature.columns]\n",
        "  # range(feature.shape[1])\n",
        "  coef=[0 if np.isnan(i) else i for i in coef]\n",
        "  return coef\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61xH6L8j32If",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#signal to noise ratio\n",
        "#using weighted one-vs-all strategy for multi-class data\n",
        "def signaltonoise(feature, target, axis = 0, ddof = 0):\n",
        "  import numpy as np\n",
        "  classes = np.unique(target)\n",
        "  if len(feature.shape)<2:\n",
        "    feature = feature.reshape(-1,1)\n",
        "  row, _ = feature.shape\n",
        "  if len(classes) <= 2:\n",
        "    m = None\n",
        "    std = 0\n",
        "    for each in classes:\n",
        "      idx = np.where(target == each)[0]\n",
        "      #convinient way of doing m1-m2\n",
        "      if m is None:\n",
        "        m = feature.iloc[idx, :].mean(axis)\n",
        "      else:\n",
        "        m -= feature.iloc[idx, :].mean(axis)\n",
        "\n",
        "      #sd1+sd2\n",
        "      std += feature.iloc[idx, :].std(axis = axis, ddof = ddof)\n",
        "\n",
        "    return np.asanyarray(m/std)\n",
        "\n",
        "  else:\n",
        "    snr_scores = [] #for storing the weighted scores\n",
        "    #using the one vs all strategy for each class with\n",
        "    for each in classes:\n",
        "      idx = np.where(target == each)[0]\n",
        "      idxn = np.where(target != each)[0]\n",
        "      m = feature.iloc[idx, :].mean(axis) - feature.iloc[idxn, :].mean(axis)\n",
        "      std = feature.iloc[idx, :].std(axis = axis, ddof = ddof) + feature.iloc[idxn, :].std(axis = axis, ddof = ddof) \n",
        "      snr_scores.append((m/std) * len(idx)/row) #weighted snr\n",
        "\n",
        "    return np.asanyarray(snr_scores).sum(axis = axis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7K7udgd35rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_ranking(score):\n",
        "    \"\"\"\n",
        "    Rank features in descending order according to fisher score, the larger the fisher score, the more important the\n",
        "    feature is\n",
        "    \"\"\"\n",
        "    idx = np.argsort(score, 0)\n",
        "    return idx[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FTz7uo23782",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "relief_score=reliefF(feature,target,NEIGHBOURS)\n",
        "\n",
        "mutual_inf=mutual_info_wrapper(feature,target)\n",
        "\n",
        "mms=MinMaxScaler()\n",
        "nfeature=mms.fit_transform(feature)\n",
        "chi_score,p_val=chi2(nfeature,target)\n",
        "\n",
        "p_corr = pearson_corr(feature, target)\n",
        "\n",
        "f_score = fisher_score(feature.values, target)\n",
        "\n",
        "tt_score = t_test(feature, target)\n",
        "\n",
        "snr_score = signaltonoise(feature, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ekDpdoJ4CZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The Features are sorted as per their scores\n",
        "sorted_relief = feature_ranking(relief_score)[:p]\n",
        "sorted_mi = feature_ranking(mutual_inf)[:p]\n",
        "sorted_chi = feature_ranking(chi_score)[:p]\n",
        "sorted_pc = feature_ranking(p_corr)[:p]\n",
        "sorted_fs = feature_ranking(f_score)[:p]\n",
        "sorted_tt = feature_ranking(tt_score)[:p]\n",
        "sorted_snr = feature_ranking(snr_score)[:p]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk1j5t3i4FG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Can Skip this Cell\n",
        "\n",
        "print(\"Features after sorting -\")\n",
        "print(\"\\nSorted MI -\",sorted_mi)\n",
        "print(\"\\nSorted Relief -\",sorted_relief)\n",
        "print(\"\\nSorted Chi -\",sorted_chi)\n",
        "print(\"\\nSorted Pearson Corr -\",sorted_pc)\n",
        "print(\"\\nSorted Fisher Score -\",sorted_fs)\n",
        "print(\"\\nSorted T-test -\",sorted_tt)\n",
        "print(\"\\nSorted SNR - \", sorted_snr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxB0NjbqGCi1",
        "colab_type": "text"
      },
      "source": [
        "## Supervised Gene Clustering\n",
        "The below cells are used for facilitating the SGC Method of Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmapYaN54Hbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score(a,p,target):  \n",
        "  if p==1:\n",
        "    return mutual_info_wrapper(pd.DataFrame(a.reshape(-1,1)),target)\n",
        "    \n",
        "  if p==2:    \n",
        "    ndf=pd.DataFrame()\n",
        "    ndf[0]=a\n",
        "    reliefa=reliefF(ndf,target,NEIGHBOURS,2)\n",
        "    return reliefa\n",
        "  \n",
        "  if p==3:    \n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    mms=MinMaxScaler() \n",
        "    a=mms.fit_transform(a.reshape(-1,1))\n",
        "    chia=chi2(a,target)[0]\n",
        "    return chia\n",
        "  \n",
        "  if p==4:\n",
        "    return pearson_corr(pd.DataFrame(a.reshape(-1, 1)), target)\n",
        "  \n",
        "  if p==5:\n",
        "    return fisher_score(a.reshape(-1,1), target)\n",
        "  \n",
        "  if p==6:\n",
        "    return t_test(a, target)\n",
        "  \n",
        "  if p==7:\n",
        "    return signaltonoise(pd.DataFrame(a.reshape(-1,1)), target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj6Up7595xHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clusters(genes,features,p,target):\n",
        "  \"\"\"\n",
        "  genes - list of subset gene. These are the genes of picked by the score function. Please note that these are just the gene names. Their actual values are passed in the features dataframe\n",
        "  features - the dataframe which contains the values of the genes\n",
        "  p - this denotes the  type of score function. 1- mutual information, 2- reliefF, 3- chi square test.\n",
        "  target - target is a pandas series of target clases for each observation\n",
        "  \"\"\"\n",
        "  clusters={}\n",
        "  cluster_gene={}\n",
        "  x,y=0,0\n",
        "  genes_copy_1=np.copy(genes)\n",
        "  while(len(genes_copy_1)>0):\n",
        "    # print(\"Starting New Iteration with\", len(genes_copy_1),\"number of genes!\")\n",
        "    genes_copy_2=np.copy(genes_copy_1)\n",
        "    r_gene=genes_copy_2[0]\n",
        "    r_gene_values=features[r_gene].values\n",
        "\n",
        "    clusters[str(r_gene)]=[]\n",
        "    \n",
        "    genes_copy_2=np.delete(genes_copy_2,0)\n",
        "    genes_copy_1=np.delete(genes_copy_1,0)\n",
        "    \n",
        "    \n",
        "    \n",
        "    r_score=score(r_gene_values,p,target)[0]\n",
        "    \n",
        "    # print(\"\\nCluster number=\",len(clusters))\n",
        "    # print(\"First feature =========================j1=\",r_gene,\"\\n\")\n",
        "    x+=1\n",
        "    # print(\"Intial Relevance Score\",r_score)\n",
        "\n",
        "    while(len(genes_copy_2)>0):\n",
        "      \n",
        "      gs=genes_copy_2[0]\n",
        "      gene=features[gs].values\n",
        "\n",
        "      y+=1      \n",
        "      \n",
        "      a_plus=np.add(r_gene_values,gene,dtype='float64') #creating A+\n",
        "      a_minus=np.subtract(r_gene_values,gene,dtype='float64') #Creating A-\n",
        "\n",
        "      a_plus_score=score(a_plus,p,target)[0]\n",
        "      a_minus_score=score(a_minus,p,target)[0]\n",
        "      \n",
        "      new_score=a_plus_score if a_plus_score>a_minus_score else a_minus_score\n",
        "      # print(\"Gene\",gs,\"+ Score\",a_plus_score,\"- Score\",a_minus_score)\n",
        "\n",
        "      if new_score>r_score:\n",
        "\n",
        "        if a_plus_score==new_score:\n",
        "\n",
        "          # print(\"Gene Under Consideration\",gs)\n",
        "          # print(\"Initial Relevance\",r_score,\"Final Relevance\",a_plus_score,r_score<a_plus_score)\n",
        "\n",
        "          clusters[str(r_gene)].append(str(gs)+\"+\")\n",
        "          r_gene_values=a_plus[:]\n",
        "          r_score=a_plus_score\n",
        "\n",
        "          # print(\"cluster member = +\",gs,\"\\tRelevance Changed to\",r_score)\n",
        "\n",
        "        elif a_minus_score==new_score:\n",
        "\n",
        "          # print(\"Gene Under Consideration\",gs)\n",
        "          # print(\"Initial Relevance\",r_score,\"Final Relevance\",a_minus_score,r_score<a_minus_score)\n",
        "          \n",
        "          clusters[str(r_gene)].append(str(gs)+\"-\")\n",
        "          r_gene_values=a_minus[:]\n",
        "          r_score=a_minus_score\n",
        "\n",
        "        #   print(\"cluster member = -\",gs,\"\\tRelevance Changed to\",r_score)\n",
        "        # print(\"Gene\",gs,\"selected!\",np.where(genes_copy_1 == gs))\n",
        "        genes_copy_1 = np.delete(genes_copy_1, np.where(genes_copy_1 == gs))      \n",
        "      genes_copy_2=np.delete(genes_copy_2,0)\n",
        "    \n",
        "    # for each in clusters[str(r_gene)]:\n",
        "    #     genes_copy_1=np.delete(genes_copy_1,np.where(genes_copy_1==each))\n",
        "    cluster_gene[r_gene]=r_gene_values\n",
        "\n",
        "  #   print(\"\\nFinal Relevance Score\",r_score)\n",
        "  print(\"Clusters formed! Returning Clusters and Gene Representatives\")\n",
        "  return clusters,cluster_gene"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2PfL-E5zqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mi_cluster, gene_repre_1 = get_clusters(sorted_mi, feature, 1, target)\n",
        "relief_cluster ,gene_repre_2 = get_clusters(sorted_relief, feature, 2, target)\n",
        "chi_cluster, gene_repre_3 = get_clusters(sorted_chi, feature, 3, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qYY9up8525B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pc_cluster, gene_repre_4 = get_clusters(sorted_pc, feature, 4, target)\n",
        "fs_cluster, gene_repre_5 = get_clusters(sorted_fs, feature, 5, target)\n",
        "tt_cluster, gene_repre_6 = get_clusters(sorted_tt, feature, 6, target)\n",
        "snr_cluster, gene_repre_7 = get_clusters(sorted_snr, feature, 7, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RoWD13d58GM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of MI Clusters formed -\",len(mi_cluster))\n",
        "print(\"Number of ReliefF Clusters formed -\",len(relief_cluster))\n",
        "print(\"Number of ChiSq. Clusters formed -\",len(chi_cluster))\n",
        "print(\"Number of Pearson Clusters formed -\",len(pc_cluster))\n",
        "print(\"Number of Fisher Score Clusters formed -\",len(fs_cluster))\n",
        "print(\"Number of T-Test Clusters formed -\",len(tt_cluster))\n",
        "print(\"Number of SNR Clusters formed -\", len(snr_cluster))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLo4_7Vy9MnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qmin = min([len(mi_cluster), len(relief_cluster), len(chi_cluster), len(pc_cluster), len(fs_cluster), len(tt_cluster), len(snr_cluster)])\n",
        "q = q if q <= qmin else qmin\n",
        "print(\"Choosing top %s Augmented Genes from each cluster\"%(q))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARuwaM3X5-YJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gene_repre_1 = pd.DataFrame(gene_repre_1)\n",
        "gene_repre_2 = pd.DataFrame(gene_repre_2)\n",
        "gene_repre_3 = pd.DataFrame(gene_repre_3)\n",
        "gene_repre_4 = pd.DataFrame(gene_repre_4)\n",
        "gene_repre_5 = pd.DataFrame(gene_repre_5)\n",
        "gene_repre_6 = pd.DataFrame(gene_repre_6)\n",
        "gene_repre_7 = pd.DataFrame(gene_repre_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gbBAexcIuVr",
        "colab_type": "text"
      },
      "source": [
        "## Saving the Gene Representatives and Clusters formed from **SGC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoKYDXrMEwTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving Gene Representatives\n",
        "gene_repre_1.to_csv(\"%s_p%s_q%sRepresentative_Genes_1.csv\"%(DATASET, p, q),index=False)\n",
        "gene_repre_2.to_csv(\"%s_p%s_q%sRepresentative_Genes_2.csv\"%(DATASET, p, q),index=False)\n",
        "gene_repre_3.to_csv(\"%s_p%s_q%sRepresentative_Genes_3.csv\"%(DATASET, p, q),index=False)\n",
        "gene_repre_4.to_csv(\"%s_p%s_q%sRepresentative_Genes_4.csv\"%(DATASET, p, q),index=False)\n",
        "gene_repre_5.to_csv(\"%s_p%s_q%sRepresentative_Genes_5.csv\"%(DATASET, p, q),index=False)\n",
        "gene_repre_6.to_csv(\"%s_p%s_q%sRepresentative_Genes_6.csv\"%(DATASET, p, q),index=False)\n",
        "gene_repre_7.to_csv(\"%s_p%s_q%sRepresentative_Genes_7.csv\"%(DATASET, p, q),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSWC_hbk6EjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Saving the clusters to JSON files, this preserves the gene selection sequence\n",
        "\"\"\"\n",
        "with open('%s_p%s_%smi_cluster.json'%(DATASET, p, q), 'w') as fp:\n",
        "    json.dump(mi_cluster, fp)\n",
        "\n",
        "with open('%s_p%s_%srelief_cluster.json'%(DATASET, p, q), 'w') as fp:\n",
        "    json.dump(relief_cluster, fp)\n",
        "\n",
        "\n",
        "with open('%s_p%s_%schi_cluster.json'%(DATASET, p, q), 'w') as fp:\n",
        "    json.dump(chi_cluster, fp)\n",
        "\n",
        "with open('%s_p%s_%spc_cluster.json'%(DATASET, p, q), 'w') as fp:\n",
        "    json.dump(pc_cluster, fp)\n",
        "\n",
        "with open('%s_p%s_%sfs_cluster.json'%(DATASET, p, q), 'w') as fp:\n",
        "    json.dump(fs_cluster, fp)\n",
        "\n",
        "with open('%s_p%s_%stt_cluster.json'%(DATASET, p, q), 'w') as fp:\n",
        "    json.dump(tt_cluster, fp)\n",
        "\n",
        "with open('%s_p%s_%ssnr_cluster.json'%(DATASET, p, q), 'w') as fp:\n",
        "    json.dump(snr_cluster, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGuxIBio6FqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_keys(scores,gene_repre,target,flag=True):\n",
        "  score_dict={}\n",
        "  x=0\n",
        "  for i in gene_repre.columns:\n",
        "    score_dict[i]=scores[x]\n",
        "    x+=1\n",
        "  return [k for k, v in sorted(score_dict.items(), key=lambda item: item[1], reverse = True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYGW_vte6HlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "feature_ranking cannot be used here because it sorts and returns the indices \n",
        "from 0-1. They need to be sorted using a different function\n",
        "\"\"\"\n",
        "sorted_mi_keys=sort_keys(mutual_info_wrapper(gene_repre_1,target),gene_repre_1,target,True)[:q]\n",
        "\n",
        "sorted_relief_keys=sort_keys(reliefF(gene_repre_2,target,k=NEIGHBOURS,repetitions=5),gene_repre_2,target,True)[:q]\n",
        "\n",
        "mms=MinMaxScaler()\n",
        "nfeature=mms.fit_transform(gene_repre_3)\n",
        "chi_score,p_val=chi2(nfeature,target)\n",
        "sorted_chi_keys=sort_keys(chi_score,gene_repre_3,target,False)[:q]\n",
        "\n",
        "sorted_pc_keys=sort_keys(pearson_corr(gene_repre_4,target),gene_repre_4,target,True)[:q]\n",
        "\n",
        "sorted_fs_keys=sort_keys(fisher_score(gene_repre_5.values,target),gene_repre_5,target,True)[:q]\n",
        "\n",
        "sorted_tt_keys=sort_keys(t_test(gene_repre_6,target),gene_repre_6,target,True)[:q]\n",
        "\n",
        "sorted_snr_keys = sort_keys(signaltonoise(gene_repre_7, target), gene_repre_7, target, True)[:q]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDzKChbl6KM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"MI cluster after sorting - \",sorted_mi_keys)\n",
        "print(\"Relief cluster after sorting - \",sorted_relief_keys)\n",
        "print(\"Chi cluster after sorting - \",sorted_chi_keys)\n",
        "print(\"Pearson cluster after sorting - \",sorted_pc_keys)\n",
        "print(\"Fisher cluster after sorting - \",sorted_fs_keys)\n",
        "print(\"T-Test cluster after sorting - \",sorted_tt_keys)\n",
        "print(\"SNR cluster after sorting - \",sorted_snr_keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJdYEt6wI7SG",
        "colab_type": "text"
      },
      "source": [
        "## Testing Classification of the Augmented Genes.\n",
        "Here the classfication accuracy is tested using **KNN, Decision Tree, Naive Bayes** and **SVM** as well as the **Ensemble** of them.\\\n",
        "\\\n",
        "Top i (where i ranges from 1 to q) are chosen from each augmented dataset of filters in each iteration. This dataset is used for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pha4l5xO81vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a Dataframe for containing the augmented gene keys\n",
        "aug_df_keys = pd.DataFrame({\"MI\":sorted_mi_keys, \"ReliefF\":sorted_relief_keys, \n",
        "                     \"Chi Sq\":sorted_chi_keys, \"Pearson\":sorted_pc_keys, \n",
        "                     \"Fisher\":sorted_fs_keys, \"tTest\":sorted_tt_keys, \n",
        "                     \"SNR\":sorted_snr_keys})\n",
        "\n",
        "aug_df_dict = {\"MI\":gene_repre_1, \"ReliefF\":gene_repre_2, \n",
        "                     \"Chi Sq\":gene_repre_3, \"Pearson\":gene_repre_4, \n",
        "                     \"Fisher\":gene_repre_5, \"tTest\":gene_repre_6, \n",
        "                     \"SNR\":gene_repre_7}\n",
        "\n",
        "# print(aug_df_keys.head())\n",
        "# print(aug_df_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DLfm02o6UfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOOCV=LeaveOneOut()\n",
        "data_KNN=KNeighborsClassifier(n_neighbors= int(feature.shape[0] ** 0.5))\n",
        "data_SVM=SVC(kernel='rbf',gamma='scale')\n",
        "data_NB=GaussianNB()\n",
        "data_Tree= DecisionTreeClassifier()\n",
        "rows=feature.shape[0]\n",
        "classifiers=[\"NB\",\"KNN\",\"Tree\",\"SVM\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCCwiZdF6XNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Iterating over filters\n",
        "for filter_name in aug_df_keys.columns:  \n",
        "  acc_matrix = pd.DataFrame()\n",
        "  for i in range(1,q+1):\n",
        "    \"\"\"\n",
        "    Make a dataframe out of i keys from the gene representatives obtained from\n",
        "    augmenting the chosen filter.\n",
        "    Than use LOOCV to measure accuracy on Train Dataset.\n",
        "    \"\"\"\n",
        "    acc=0\n",
        "    individual_acc = np.zeros(4)\n",
        "    cluster_df = aug_df_dict[filter_name].iloc[:, :i]\n",
        "\n",
        "    # print(cluster_df.shape)\n",
        "\n",
        "    for train_index,test_index in LOOCV.split(cluster_df):\n",
        "      \"\"\"\n",
        "      Data is divided into train-test splits and then polling method is used \n",
        "      to find the classification results (ensemble of KNN,SVM,NB,Decision Tree)\n",
        "      \"\"\"\n",
        "      train_data,train_labels=cluster_df.iloc[train_index,:],target[train_index]\n",
        "      test_data,test_labels=cluster_df.iloc[test_index,:],target[test_index].values.tolist()[0]\n",
        "      data_KNN.fit(train_data,train_labels)\n",
        "      data_SVM.fit(train_data,train_labels)\n",
        "      data_NB.fit(train_data,train_labels)\n",
        "      data_Tree.fit(train_data,train_labels)\n",
        "\n",
        "      class_list = [data_NB, data_KNN, data_Tree, data_SVM]\n",
        "      results=[]\n",
        "\n",
        "      #getting individual results\n",
        "      for x in range(4):\n",
        "        tem_result = class_list[x].predict(test_data)[0]\n",
        "        if tem_result == test_labels:\n",
        "          individual_acc[x]+=1\n",
        "        results.append(tem_result)\n",
        "      polling_result=0\n",
        "      max_freq=0\n",
        "\n",
        "      #getting ensemble results\n",
        "      for x in results:\n",
        "        freq=results.count(x)\n",
        "        if freq>max_freq:\n",
        "          max_freq=freq\n",
        "          polling_result=x\n",
        "      if polling_result == test_labels:\n",
        "        acc+=1\n",
        "\n",
        "    individual_acc = np.round(individual_acc/cluster_df.shape[0],4)\n",
        "    individual_acc = np.append(individual_acc, np.round(acc/cluster_df.shape[0],4))\n",
        "    # print(individual_acc)\n",
        "\n",
        "    acc_matrix[i] = individual_acc\n",
        "  acc_matrix = acc_matrix.T\n",
        "\n",
        "  acc_matrix.columns = classifiers[:]+['Ensemble']\n",
        "\n",
        "  print(\"\\nFilter:-\",filter_name, \"\\n\",acc_matrix)\n",
        "\n",
        "  acc_matrix.to_csv(\"Uni-SGC-%s_%s_p%s_q%s_Accuracy_Matrix.csv\"%(DATASET, filter_name, p, q))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1yxeQ8Q_4vI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for filter_name in aug_df_keys.columns:\n",
        "  files.download(\"Uni-SGC-%s_%s_p%s_q%s_Accuracy_Matrix.csv\"%(DATASET, filter_name, p, q))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VFVhovZFXeG",
        "colab_type": "text"
      },
      "source": [
        "## Loading Gene Representatives and Clusters\n",
        "The below cells can be run to load gene representatives and clusters if you already have them prepared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDmHO2BECa18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Loading the Cluster JSON files from memory\n",
        "\"\"\"\n",
        "\n",
        "with open('%s_p%s_%smi_cluster.json'%(DATASET, p, q), 'r') as fp:\n",
        "  mi_cluster=json.load(fp)\n",
        "\n",
        "with open('%s_p%s_%srelief_cluster.json'%(DATASET, p, q), 'r') as fp:\n",
        "  chi_cluster=json.load(fp)\n",
        "\n",
        "\n",
        "with open('%s_p%s_%schi_cluster.json'%(DATASET, p, q), 'r') as fp:\n",
        "  relief_cluster=json.load(fp)\n",
        "\n",
        "with open('%s_p%s_%spc_cluster.json'%(DATASET, p, q), 'r') as fp:\n",
        "  pc_cluster=json.load(fp)\n",
        "\n",
        "with open('%s_p%s_%sfs_cluster.json'%(DATASET, p, q), 'r') as fp:\n",
        "  fs_cluster=json.load(fp)\n",
        "\n",
        "with open('%s_p%s_%stt_cluster.json'%(DATASET, p, q), 'r') as fp:\n",
        "  tt_cluster=json.load(fp)\n",
        "\n",
        "with open('%s_p%s_%ssnr_cluster.json'%(DATASET, p, q), 'r') as fp:\n",
        "  snr_cluster=json.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPLw6jtJFrCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Loading Representative Genes from Memory\n",
        "\"\"\"\n",
        "gene_repre_1 = pd.read_csv(\"%s_p%s_q%sRepresentative_Genes_1.csv\"%(DATASET, p, q),index_col = None)\n",
        "gene_repre_2 = pd.read_csv(\"%s_p%s_q%sRepresentative_Genes_2.csv\"%(DATASET, p, q),index_col = None)\n",
        "gene_repre_3 = pd.read_csv(\"%s_p%s_q%sRepresentative_Genes_3.csv\"%(DATASET, p, q),index_col = None)\n",
        "gene_repre_4 = pd.read_csv(\"%s_p%s_q%sRepresentative_Genes_4.csv\"%(DATASET, p, q),index_col = None)\n",
        "gene_repre_5 = pd.read_csv(\"%s_p%s_q%sRepresentative_Genes_5.csv\"%(DATASET, p, q),index_col = None)\n",
        "gene_repre_6 = pd.read_csv(\"%s_p%s_q%sRepresentative_Genes_6.csv\"%(DATASET, p, q),index_col = None)\n",
        "gene_repre_7 = pd.read_csv(\"%s_p%s_q%sRepresentative_Genes_7.csv\"%(DATASET, p, q),index_col = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21CVfMTSF2g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}