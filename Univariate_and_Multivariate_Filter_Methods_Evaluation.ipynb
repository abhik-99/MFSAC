{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Univariate and Multivariate Filter Methods Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhik-99/MFSGC/blob/master/Univariate_and_Multivariate_Filter_Methods_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N64u582EzHC_",
        "colab_type": "text"
      },
      "source": [
        "# Testing of Filter Methods (Univariate)\n",
        "\n",
        "The First Part of this notebook evaluates how a single filtering method performs against **KNN, SVM, Decision Tree and Naive Bayes Algorithm** as well as **Ensemble**.\\\n",
        "At each step, a particular filtering method is chosen and then it is evaluated.\\\n",
        "\\\n",
        "In the second part, multiple filtering methods are taken for scoring the features. Top *n* features from each filtered dataset is taken and performance is analysed against the same classification algorithms.\\\n",
        "\\\n",
        "**Leave One Out Cross Validation (LOOCV)** is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pOJTfnU0Wd_",
        "colab_type": "text"
      },
      "source": [
        "## General Steps before evaluation\n",
        "The cells below are same for univariate and multivariate filter method evaluation. As such, they may be executed only once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI-xRI4N3Gk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import chi2\n",
        "!pip install -U -q PyDrive\n",
        "!pip install skfeature-chappers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGfaDqh_3OTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2. Get the file\n",
        "downloaded = drive.CreateFile({'id':'1oaOATE0D_f8MGPIMJOMXYVt0hBUWNCKV'}) # replace the id with id of file you want to access\n",
        "# For Leukemia- 1xcL-LT-E_gUqWLlqqeVJP1DVHHpiAGe_\n",
        "# For Colon - 1AUOto0GhTHW9fX52XSsf9kzYJS5ggv0G\n",
        "# for Prostate - 13Hf7uGbyJ1sWYo8KDRDL8scm-2Fs9_gd\n",
        "# For Lung- 1xuLzTWDGUbr4x3Pq1dnJj08MZqBB5I3U\n",
        "# for Rahc - 1oaOATE0D_f8MGPIMJOMXYVt0hBUWNCKV\n",
        "# for Raoa - 1d2vhPcT3I7ZFcAGOQYVLGB3Jx_vEMata\n",
        "# for Rbreast - 1Vf-h8zfVP_twMXivcJJtbWtjThShUHvn\n",
        "# for SRBCT - 1rO5EEvsoRJl2VVUB3ywKUd3kNiQ24oy3\n",
        "# for MLL - 1rS7x4x_DhrUzaBhrgKMQH3uIaLJdPgW3\n",
        "# for Breast - 1enhhyA4u2ByvOjnF81WoHflVNpXtfKpu\n",
        "downloaded.GetContentFile('data.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61UeZR6Z3UZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DATASET is the name of the dataset being used.\n",
        "DATASET=\"RAHC\"\n",
        "\n",
        "#NEIGHBOURS determines neighbours arg for ReliefF\n",
        "#for any dataset which contains any class sample \n",
        "# <10, make it less than 10. Eg of such dataset - SRBCT\n",
        "NEIGHBOURS = 3 \n",
        "\n",
        "#p is the number of top genes taken after sorting the filter scores\n",
        "p = 5\n",
        "\n",
        "#uncomment the line below if using the dataset splitter else leave it commented \n",
        "#data_df = pd.read_csv(\"%s_train.csv\"%(DATASET),index_col=0)\n",
        "\n",
        "#uncomment the lines below if using the original dataset\n",
        "dataset = pd.read_table(\"data.txt\",header=None)\n",
        "data_df = dataset\n",
        "\n",
        "\n",
        "\n",
        "target = data_df.iloc[:,-1]\n",
        "feature = pd.DataFrame(data_df.iloc[:,:-1].values,dtype='float')\n",
        "m,n = feature.shape\n",
        "print(m,n)\n",
        "print(feature.head())\n",
        "print(\"Number of classes - \")\n",
        "classes = np.unique(target)\n",
        "for x in classes:\n",
        "  print(\"Class -\",x,\"Number of Sampples -\", len(np.where(target == x)[0]))\n",
        "\n",
        "feature_norm=pd.DataFrame(MinMaxScaler().fit_transform(feature))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ejz5sjk3c66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#construction of ReliefF function\n",
        "\n",
        "\"\"\"\n",
        "Given a dataset, number of random instances to pick form the dataset and\n",
        "number of features to consider in each iteration (k), the function returns the weigths of the attributes\n",
        "of the dataset.\n",
        "These weigths can then be used as the final results out of the ReliefF algorithm\n",
        "\n",
        "Paper-\n",
        "\n",
        "Marko Robnik-ˇSikonja and Igor Kononenko. Theoretical and empirical analysis of relieff\n",
        "and rrelieff. Machine learning, 53(1-2):23–69, 2003.\n",
        "\n",
        "\"\"\"\n",
        "def reliefF(feature,targetClass,n_neighbours=10,instances_to_select=10):\n",
        "  import numpy as np\n",
        "  \n",
        "  #feature=df.iloc[:,:-1]\n",
        "  #targetClass=df.iloc[:,-1]\n",
        "  m,n=feature.shape\n",
        "  #print(\"Dimensions:\",m,n)\n",
        "  weights=np.zeros(n,dtype='int')\n",
        "\n",
        "  classes=np.unique(targetClass)\n",
        "\n",
        "  m2=instances_to_select #number of features to pickup randomly from the \n",
        "  k=n_neighbours #number of neighbours to consider\n",
        "\n",
        "  instances=np.array(list(range(1,m)))\n",
        "  \n",
        "\n",
        "  minimums=np.min(feature.values,axis=0)\n",
        "  maximums=np.max(feature.values,axis=0)\n",
        "\n",
        "  differ=np.subtract(maximums,minimums)\n",
        "\n",
        "\n",
        "  for i in range(m2):\n",
        "    chosen=np.random.choice(instances[:-1])\n",
        "    instances=np.delete(instances,np.where(instances==chosen))\n",
        "\n",
        "    rI=feature.iloc[chosen,:].values\n",
        "\n",
        "    instanceClass=targetClass[chosen]\n",
        "    probIClass=len(np.where(targetClass==instanceClass)[0])/m #getting the probaility of choosing this class\n",
        "\n",
        "    hit=[]\n",
        "    miss={}\n",
        "\n",
        "    low,high,tem1,tem2=0,m,(chosen-1),(chosen+1)\n",
        "    hitFlag=True\n",
        "    missFlag=True\n",
        "    \n",
        "    \n",
        "    while(hitFlag==True):\n",
        "\n",
        "      if targetClass[tem1]==instanceClass:\n",
        "        hit.append(tem1)\n",
        "      if targetClass[tem2]==instanceClass:\n",
        "        hit.append(tem2)\n",
        "      if len(hit)==k:\n",
        "        hitFlag=False\n",
        "      if tem1>0:\n",
        "        tem1-=1\n",
        "      if tem2<m-1:\n",
        "        tem2+=1\n",
        "    for x in classes:\n",
        "      if x==instanceClass:\n",
        "        continue\n",
        "      #print(instanceClass,x)\n",
        "      cli=max(np.where(targetClass==x)[0]) #finding the last instance of the class x\n",
        "      if cli>chosen:\n",
        "        tem=min(np.where(targetClass==x)[0])\n",
        "        miss[x]=list(range(tem,tem+k))\n",
        "      else:\n",
        "        tem=max(np.where(targetClass==x)[0])\n",
        "        miss[x]=list(range(cli-k+1,cli))\n",
        "\n",
        "    #print(\"Chosen-\",chosen,\"Hits-\",hit,\"Misses-\",miss)\n",
        "\n",
        "    totalHit=np.zeros(n,dtype='int')\n",
        "\n",
        "    for hit in range(k):\n",
        "      hI=feature.iloc[hit,:].values\n",
        "      dRH=np.divide(np.abs(np.subtract(rI,hI)),differ)\n",
        "      dRH=dRH/(m2*k)\n",
        "      totalHit=np.add(totalHit,dRH)\n",
        "\n",
        "    totalMiss=np.zeros(n,dtype='int')\n",
        "\n",
        "    for eachClass in miss:\n",
        "\n",
        "      tMiss=np.zeros(n,dtype='int')\n",
        "      pclass=len(np.where(targetClass==eachClass)[0])/m #getting the probability of getting this class\n",
        "      postProb=pclass/(1-probIClass) #calculating the posterior probanility of getting this class\n",
        "\n",
        "      for eachMiss in miss[eachClass]:\n",
        "        mI=feature.iloc[eachMiss,:].values\n",
        "        dRM=np.divide(np.abs(np.subtract(rI,mI)),differ)\n",
        "        dRM=dRM/(m2*k)\n",
        "        tMiss=np.add(tMiss,dRM)\n",
        "\n",
        "      totalMiss=np.add(totalMiss,(tMiss*postProb))\n",
        "\n",
        "    weights=np.add(weights,totalMiss)\n",
        "    weights=np.subtract(weights,totalHit) \n",
        "    \n",
        "  return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyBPMqTI3hmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function discretizes the given features into 3 categories\n",
        "def discretize_feature(feature):\n",
        "  import numpy as np\n",
        "  mean=np.mean(feature)\n",
        "  std=np.std(feature)\n",
        "  discretized=np.copy(feature)\n",
        "  \n",
        "  discretized[np.where(feature<(mean+std/2)) ,]=2#within 1/2 std div\n",
        "  discretized[np.where(feature>(mean-std/2)),]=2#within 1/2 std div\n",
        "  \n",
        "  discretized[np.where(feature>(mean+std/2)),]=0#greater than half\n",
        "  discretized[np.where(feature<(mean-std/2)),]=1#less than half\n",
        "  \n",
        "  return discretized\n",
        "\n",
        "def Xfreq(x):\n",
        "  xL={}\n",
        "  for e in x:\n",
        "    if e not in xL:\n",
        "      xL[e]=0\n",
        "    else:\n",
        "      xL[e]+=1\n",
        "  for e in xL:\n",
        "    xL[e]/=len(x)\n",
        "  return xL\n",
        "\n",
        "def XYfreq(x,y):\n",
        "  freq={}\n",
        "  import numpy as np\n",
        "  \n",
        "  rX=np.unique(x)\n",
        "  rY=np.unique(y)\n",
        "      \n",
        "  for e in rX:\n",
        "    for f in rY:\n",
        "      freq[(e,f)]=round(len(np.where(y[np.where(x==e)[0]]==f)[0])/len(x),4)\n",
        "       \n",
        "  return freq\n",
        "\n",
        "def mutual_info(x,y):\n",
        "  import numpy as np\n",
        "  \n",
        "  xFreq=Xfreq(x)\n",
        "  yFreq=Xfreq(y)\n",
        "  joint=XYfreq(x,y)\n",
        "  \n",
        "  Xentropy=0\n",
        "  for e in xFreq:\n",
        "    if xFreq[e]!=0:\n",
        "      Xentropy-=xFreq[e]*np.log2(xFreq[e])\n",
        "      \n",
        "  Yentropy=0\n",
        "  for e in yFreq:\n",
        "    if yFreq[e]!=0:\n",
        "      Yentropy-=yFreq[e]*np.log2(yFreq[e])\n",
        "      \n",
        "  jentropy=0\n",
        "  for e in xFreq:\n",
        "    for f in yFreq:\n",
        "      if joint[(e,f)]!=0:\n",
        "        jentropy-=joint[(e,f)]*np.log2(joint[(e,f)])\n",
        "  \n",
        "  return (Xentropy+Yentropy-jentropy)\n",
        "\n",
        "def mutual_info_wrapper(features,targetClass):\n",
        "  import numpy as np\n",
        "  mi=np.array([])\n",
        "  for x in features:\n",
        "    discrete=discretize_feature(features[x])\n",
        "    mi=np.append(mi,mutual_info(discrete,targetClass))\n",
        "  return np.array(mi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NwXzIg3WJll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This cell is used for getting the t-test scores\n",
        "\"\"\"\n",
        "\n",
        "def t_test(df,target):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  df= Dataframe of features (n_samples,n_features)\n",
        "  target= Pandas Series/1D Numpy Array containing the class labels (n_samples)\n",
        "  \n",
        "  Output:\n",
        "  scores= Descendingly Sorted array of features based on t-test \n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  from scipy.stats import ttest_ind\n",
        "  scores=ttest_ind(df[:][targetClass==0],df[:][targetClass==1])[0] #Storing just the t-test scores and discarding the p-values from the result.\n",
        "  \n",
        "  scores=np.argsort(scores,0)\n",
        "  \n",
        "  return scores[::-1]\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGJb9lDPRaaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import *\n",
        "def fisher_score(X, y):\n",
        "    import numpy as np\n",
        "    \n",
        "    from skfeature.utility.construct_W import construct_W\n",
        "    \"\"\"\n",
        "    This function implements the fisher score feature selection, steps are as follows:\n",
        "    1. Construct the affinity matrix W in fisher score way\n",
        "    2. For the r-th feature, we define fr = X(:,r), D = diag(W*ones), ones = [1,...,1]', L = D - W\n",
        "    3. Let fr_hat = fr - (fr'*D*ones)*ones/(ones'*D*ones)\n",
        "    4. Fisher score for the r-th feature is score = (fr_hat'*D*fr_hat)/(fr_hat'*L*fr_hat)-1\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    X: {numpy array}, shape (n_samples, n_features)\n",
        "        input data\n",
        "    y: {numpy array}, shape (n_samples,)\n",
        "        input class labels\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    score: {numpy array}, shape (n_features,)\n",
        "        fisher score for each feature\n",
        "\n",
        "    Reference\n",
        "    ---------\n",
        "    He, Xiaofei et al. \"Laplacian Score for Feature Selection.\" NIPS 2005.\n",
        "    Duda, Richard et al. \"Pattern classification.\" John Wiley & Sons, 2012.\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct weight matrix W in a fisherScore way\n",
        "    kwargs = {\"neighbor_mode\": \"supervised\", \"fisher_score\": True, 'y': y}\n",
        "    W = construct_W(X, **kwargs)\n",
        "\n",
        "    # build the diagonal D matrix from affinity matrix W\n",
        "    D = np.array(W.sum(axis=1))\n",
        "    L = W\n",
        "    tmp = np.dot(np.transpose(D), X)\n",
        "    D = diags(np.transpose(D), [0])\n",
        "    Xt = np.transpose(X)\n",
        "    t1 = np.transpose(np.dot(Xt, D.todense()))\n",
        "    t2 = np.transpose(np.dot(Xt, L.todense()))\n",
        "    # compute the numerator of Lr\n",
        "    D_prime = np.sum(np.multiply(t1, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
        "    # compute the denominator of Lr\n",
        "    L_prime = np.sum(np.multiply(t2, X), 0) - np.multiply(tmp, tmp)/D.sum()\n",
        "    # avoid the denominator of Lr to be 0\n",
        "    D_prime[D_prime < 1e-12] = 10000\n",
        "    lap_score = 1 - np.array(np.multiply(L_prime, 1/D_prime))[0, :]\n",
        "\n",
        "    # compute fisher score from laplacian score, where fisher_score = 1/lap_score - 1\n",
        "    score = 1.0/lap_score - 1\n",
        "    return np.transpose(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9LQMJetyCYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#signal to noise ratio\n",
        "#using weighted one-vs-all strategy for multi-class data\n",
        "def signaltonoise(feature, target, axis = 0, ddof = 0):\n",
        "  import numpy as np\n",
        "  classes = np.unique(target)\n",
        "  if len(feature.shape)<2:\n",
        "    feature = feature.reshape(-1,1)\n",
        "  row, _ = feature.shape\n",
        "  if len(classes) <= 2:\n",
        "    m = None\n",
        "    std = 0\n",
        "    for each in classes:\n",
        "      idx = np.where(target == each)[0]\n",
        "      #convinient way of doing m1-m2\n",
        "      if m is None:\n",
        "        m = feature.iloc[idx, :].mean(axis)\n",
        "      else:\n",
        "        m -= feature.iloc[idx, :].mean(axis)\n",
        "\n",
        "      #sd1+sd2\n",
        "      std += feature.iloc[idx, :].std(axis = axis, ddof = ddof)\n",
        "\n",
        "    return np.asanyarray(m/std)\n",
        "\n",
        "  else:\n",
        "    snr_scores = [] #for storing the weighted scores\n",
        "    #using the one vs all strategy for each class with\n",
        "    for each in classes:\n",
        "      idx = np.where(target == each)[0]\n",
        "      idxn = np.where(target != each)[0]\n",
        "      m = feature.iloc[idx, :].mean(axis) - feature.iloc[idxn, :].mean(axis)\n",
        "      std = feature.iloc[idx, :].std(axis = axis, ddof = ddof) + feature.iloc[idxn, :].std(axis = axis, ddof = ddof) \n",
        "      snr_scores.append((m/std) * len(idx)/row) #weighted snr\n",
        "\n",
        "    return np.asanyarray(snr_scores).sum(axis = axis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82vk2iNniu_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Gini index, forked from oliviaguest from GitHub.\n",
        "\n",
        "def gini(array):\n",
        "    import numpy as np\n",
        "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
        "    # based on bottom eq:\n",
        "    # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n",
        "    # from:\n",
        "    # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
        "    # All values are treated equally, arrays must be 1d:\n",
        "    array = array.flatten()\n",
        "    if np.amin(array) < 0:\n",
        "        # Values cannot be negative:\n",
        "        array -= np.amin(array)\n",
        "    # Values cannot be 0:\n",
        "    array += 0.0000001\n",
        "    # Values must be sorted:\n",
        "    array = np.sort(array)\n",
        "    # Index per array element:\n",
        "    index = np.arange(1,array.shape[0]+1)\n",
        "    # Number of array elements:\n",
        "    n = array.shape[0]\n",
        "    # Gini coefficient:\n",
        "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))\n",
        "def gini_wrapper(features):\n",
        "  return np.argsort([gini(features[x].values) for x in range(features.shape[1])],0).tolist()[::-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNEoi-BNvKTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pearson_corr(feature,targetClass):\n",
        "  import numpy as np\n",
        "  coef=[np.abs(np.corrcoef(feature[i].values,targetClass)[0,1]) for i in range(feature.shape[1])]\n",
        "  coef=[0 if np.isnan(i) else i for i in coef]\n",
        "  return np.argsort(coef,0).tolist()[::-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by5p38PoyE9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def feature_ranking(score):\n",
        "    \"\"\"\n",
        "    Rank features in descending order according to fisher score, the larger the fisher score, the more important the\n",
        "    feature is\n",
        "    \"\"\"\n",
        "    idx = np.argsort(score, 0)\n",
        "    return idx[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_-usGj43oqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "relief_score=reliefF(feature,target,NEIGHBOURS)\n",
        "\n",
        "mutual_inf=mutual_info_wrapper(feature,target)\n",
        "\n",
        "mms=MinMaxScaler()\n",
        "nfeature=mms.fit_transform(feature)\n",
        "chi_score,p_val=chi2(nfeature,target)\n",
        "\n",
        "p_corr = pearson_corr(feature, target)\n",
        "\n",
        "f_score = fisher_score(feature.values, target)\n",
        "\n",
        "tt_score = t_test(feature, target)\n",
        "\n",
        "snr_score = signaltonoise(feature, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NcaZI2m79Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The Features are sorted as per their scores\n",
        "sorted_relief = feature_ranking(reliefF(feature,target,NEIGHBOURS))[:p]\n",
        "\n",
        "sorted_mi = feature_ranking(mutual_info_wrapper(feature,target))[:p]\n",
        "\n",
        "mms=MinMaxScaler()\n",
        "nfeature=mms.fit_transform(feature)\n",
        "chi_score,_=chi2(nfeature,target)\n",
        "sorted_chi = feature_ranking(chi_score)[:p]\n",
        "\n",
        "sorted_pc = feature_ranking(pearson_corr(feature, target))[:p]\n",
        "sorted_fs = feature_ranking(fisher_score(feature.values, target))[:p]\n",
        "sorted_tt = feature_ranking(t_test(feature, target))[:p]\n",
        "sorted_snr = feature_ranking(signaltonoise(feature, target))[:p]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t-zzL9Dy089",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOOCV=LeaveOneOut()\n",
        "data_KNN=KNeighborsClassifier(n_neighbors= int(feature.shape[0] ** 0.5))\n",
        "data_SVM=SVC(kernel='rbf',gamma='scale')\n",
        "data_NB=GaussianNB()\n",
        "data_Tree= DecisionTreeClassifier()\n",
        "rows=feature.shape[0]\n",
        "classifiers=[\"NB\",\"KNN\",\"Tree\",\"SVM\"]\n",
        "\n",
        "keys_list=[sorted_mi_keys, sorted_relief_keys, sorted_chi_keys, sorted_pc_keys, \n",
        "           sorted_fs_keys, sorted_tt_keys, sorted_snr_keys]\n",
        "cluster_list=[gene_repre_1, gene_repre_2, gene_repre_3, gene_repre_4, \n",
        "              gene_repre_5, gene_repre_6, gene_repre_7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkFRa3r00qA5",
        "colab_type": "text"
      },
      "source": [
        "## **PART 1**\n",
        "Univariate Filter Method Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWl99nI60z_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_matrix = pd.DataFrame()\n",
        "for i in range(1,6):\n",
        "  \"\"\"\n",
        "  Make a dataframe out of i keys from each gene_representatives from their \n",
        "  respective sorted keys. \n",
        "  Than use LOOCV to measure accuracy on Train Dataset.\n",
        "  \"\"\"\n",
        "  cluster_df=pd.DataFrame()\n",
        "  s=0\n",
        "  for x in range(len(cluster_list)):\n",
        "    tem_df = cluster_list[x][keys_list[x][:i]]\n",
        "    for x in range(i):\n",
        "      cluster_df[s] = tem_df.iloc[:,x]\n",
        "      s+=1\n",
        "  print(cluster_df.shape)\n",
        "  acc=0\n",
        "  individual_acc = np.zeros(4)\n",
        "  \n",
        "  for train_index,test_index in LOOCV.split(cluster_df):\n",
        "    \"\"\"\n",
        "    Data is divided into train-test splits and then polling method is used \n",
        "    to find the classification results (ensemble of KNN,SVM,NB,Decision Tree)\n",
        "    \"\"\"\n",
        "    train_data,train_labels=cluster_df.iloc[train_index,:],target[train_index]\n",
        "    test_data,test_labels=cluster_df.iloc[test_index,:],target[test_index].values.tolist()[0]\n",
        "    data_KNN.fit(train_data,train_labels)\n",
        "    data_SVM.fit(train_data,train_labels)\n",
        "    data_NB.fit(train_data,train_labels)\n",
        "    data_Tree.fit(train_data,train_labels)\n",
        "\n",
        "    class_list = [data_NB, data_KNN, data_Tree, data_SVM]\n",
        "    results=[]\n",
        "    for x in range(4):\n",
        "      tem_result = class_list[x].predict(test_data)[0]\n",
        "      if tem_result == test_labels:\n",
        "        individual_acc[x]+=1\n",
        "      results.append(tem_result)\n",
        "    polling_result=0\n",
        "    max_freq=0\n",
        "    for x in results:\n",
        "      freq=results.count(x)\n",
        "      if freq>max_freq:\n",
        "        max_freq=freq\n",
        "        polling_result=x\n",
        "    if polling_result == test_labels:\n",
        "      acc+=1\n",
        "  individual_acc = np.round(individual_acc/cluster_df.shape[0],4)\n",
        "  individual_acc = np.append(individual_acc, np.round(acc/cluster_df.shape[0],4))\n",
        "  print(individual_acc)\n",
        "  acc_matrix[i] = individual_acc\n",
        "acc_matrix = acc_matrix.T\n",
        "acc_matrix.columns = classifiers[:]+['Ensemble']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vemN5lCORoDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(acc_matrix)\n",
        "acc_matrix.to_csv(\"%s_p%sAccuracy_Matrix.csv\"%(DATASET, p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFfhBdnv0zjT",
        "colab_type": "text"
      },
      "source": [
        "## **PART 2**\n",
        "Multivariate Filter Method Evaluation"
      ]
    }
  ]
}